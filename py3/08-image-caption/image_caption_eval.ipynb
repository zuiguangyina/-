{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Loads vocab\n",
    "2. Builds data generator\n",
    "3. Builds image caption model\n",
    "4. Train\n",
    "4. Eval\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "input_description_file = \"../../image_caption_data/results_20130124.token\"\n",
    "input_img_feature_dir = \"../../image_caption_data/feature_extraction_inception_v3\"\n",
    "input_vocab_file = \"../../image_caption_data/vocab.txt\"\n",
    "output_dir = \"../../image_caption_data/local_run\"\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=3,\n",
    "        num_embedding_nodes=32,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[64, 64],\n",
    "        num_lstm_layers=2,\n",
    "        num_fc_nodes=32,\n",
    "        batch_size=1,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=0.8,\n",
    "        log_frequent=100,\n",
    "        save_frequent=1000,\n",
    "    )\n",
    "\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['2339164918.jpg',\n",
      " '197688193.jpg',\n",
      " '4553689997.jpg',\n",
      " '2774501339.jpg',\n",
      " '5510989691.jpg',\n",
      " '5897700952.jpg',\n",
      " '1432734769.jpg',\n",
      " '3225058391.jpg',\n",
      " '1035019794.jpg',\n",
      " '6286343422.jpg']\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['2339164918.jpg',\n",
      " '197688193.jpg',\n",
      " '4553689997.jpg',\n",
      " '2774501339.jpg',\n",
      " '5897700952.jpg',\n",
      " '1432734769.jpg',\n",
      " '3225058391.jpg',\n",
      " '3453369116.jpg',\n",
      " '6286343422.jpg',\n",
      " '5332049234.jpg']\n",
      "[[3, 9, 4, 132, 8, 3596, 6, 1, 48, 338, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "\n",
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses token file.\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts tokens of each description of imgs to id. \"\"\"\n",
    "    img_name_to_token_ids = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_token_ids.setdefault(img_name, [])\n",
    "        descriptions = img_name_to_tokens[img_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_token_ids[img_name].append(token_ids)\n",
    "    return img_name_to_token_ids\n",
    "\n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "    \n",
    "    \n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_token_ids = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:10])\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_ids))\n",
    "pprint.pprint(list(img_name_to_token_ids.keys())[0:10])\n",
    "pprint.pprint(img_name_to_token_ids['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../image_caption_data/feature_extraction_inception_v3/image_features-23.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-93.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-83.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-24.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-65.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-79.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-15.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-85.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-68.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-127.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-38.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-86.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-39.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-115.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-27.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-84.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-67.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-66.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-25.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-0.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-26.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-57.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-113.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-62.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-106.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-88.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-95.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-103.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-98.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-100.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-49.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-5.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-20.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-6.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-7.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-104.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-43.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-116.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-14.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-101.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-122.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-120.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-37.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-110.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-77.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-125.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-128.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-1.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-48.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-11.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-129.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-3.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-34.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-80.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-81.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-105.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-58.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-28.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-74.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-73.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-69.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-59.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-13.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-30.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-109.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-130.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-91.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-33.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-45.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-75.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-46.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-40.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-71.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-53.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-52.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-4.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-10.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-21.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-72.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-99.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-35.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-119.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-17.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-8.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-54.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-51.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-96.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-126.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-70.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-31.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-124.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-132.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-55.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-63.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-60.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-76.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-64.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-29.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-114.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-107.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-108.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-97.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-22.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-56.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-133.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-118.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-9.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-18.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-2.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-87.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-123.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-121.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-12.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-117.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-134.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-82.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-41.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-78.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-36.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-94.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-61.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-89.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-112.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-111.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-50.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-16.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-90.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-32.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-44.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-92.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-102.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-131.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-19.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-47.pickle',\n",
      " '../../image_caption_data/feature_extraction_inception_v3/image_features-42.pickle']\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-23.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-93.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-83.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-24.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-65.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-79.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-15.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-85.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-68.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-127.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-38.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-86.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-39.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-115.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-27.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-84.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-67.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-66.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-25.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-0.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-26.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-57.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-113.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-62.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-106.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-88.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-95.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-103.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-98.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-100.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-49.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-5.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-20.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-6.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-7.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-104.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-43.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-116.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-14.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-101.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-122.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-120.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-37.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-110.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-77.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-125.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-128.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-1.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-48.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-11.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-129.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-3.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-34.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-80.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-81.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-105.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-58.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-28.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-74.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-73.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-69.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-59.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-13.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-30.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-109.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-130.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-91.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-33.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-45.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-75.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-46.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-40.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-71.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-53.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-52.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-4.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-10.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-21.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-72.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-99.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-35.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-119.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-17.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-8.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-54.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-51.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-96.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-126.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-70.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-31.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-124.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-132.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-55.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-63.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-60.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-76.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-64.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-29.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-114.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-107.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-108.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-97.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-22.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-56.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-133.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-118.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-9.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-18.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-2.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-87.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-123.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-121.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-12.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-117.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-134.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-82.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-41.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-78.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-36.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-94.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-61.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-89.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-112.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-111.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-50.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-16.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-90.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-32.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-44.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-92.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-102.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-131.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-19.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-47.pickle\n",
      "INFO:tensorflow:loading ../../image_caption_data/feature_extraction_inception_v3/image_features-42.pickle\n",
      "(13500, 2048)\n",
      "(13500,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 13500\n",
      "array([[0.5972812 , 0.24378417, 0.26141885, ..., 0.13594168, 0.3063635 ,\n",
      "        0.87549466],\n",
      "       [0.6052547 , 0.1980454 , 0.01037374, ..., 0.16081782, 0.        ,\n",
      "        0.02744393],\n",
      "       [0.06349646, 0.13544098, 0.1552519 , ..., 1.0551926 , 0.35864833,\n",
      "        0.07798674],\n",
      "       [0.36298764, 0.17464292, 0.05405442, ..., 0.02735466, 0.9460683 ,\n",
      "        0.8115097 ],\n",
      "       [0.09579825, 0.04836945, 0.16781375, ..., 0.2419402 , 0.19809088,\n",
      "        0.3057389 ]], dtype=float32)\n",
      "array([[   16,   394,   223,    12,    46,     4,    20,     7,    22,\n",
      "           12],\n",
      "       [    3,   212,   474,     6,     1,     0,   156,    54,    27,\n",
      "         4714],\n",
      "       [10717,   183,     1,   365,  1296,    18,   644,   320,   559,\n",
      "           42],\n",
      "       [    3,  2027,  1133,  2043,    60,    30,    29,     6,     1,\n",
      "            9],\n",
      "       [    3,     9,     4,     1,    20,    21,     7,     1,    13,\n",
      "            4]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "array(['4061778099.jpg', '467262667.jpg', '6082254728.jpg',\n",
      "       '4613262585.jpg', '236557331.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 img_name_to_token_ids,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "\n",
    "        self._img_name_to_token_ids = img_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._deterministic = deterministic\n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        self._load_img_feature_pickle()\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def _load_img_feature_pickle(self):\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f, encoding='iso-8859-1')\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data, (origin_shape[0], origin_shape[3]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "\n",
    "    def _img_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._img_name_to_token_ids[filename]\n",
    "            # chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids = token_ids_set[0]\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        batch_img_names = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_img_names)\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_img_names\n",
    "\n",
    "\n",
    "caption_data = ImageCaptionData(img_name_to_token_ids, input_img_feature_dir, hps.num_timesteps, vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "def eval_get_embedding_for_img(hps, img_feature_dim):\n",
    "    img_feature  = tf.placeholder(tf.float32, (1, img_feature_dim))\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=img_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        return img_feature, embed_img\n",
    "\n",
    "def eval_embedding_lookup(hps, vocab_size):\n",
    "    word = tf.placeholder(tf.int32, (1, 1))\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_word = tf.nn.embedding_lookup(embeddings, word)\n",
    "    return word, embed_word\n",
    "\n",
    "def eval_lstm_single_step(hps, vocab_size):\n",
    "    embed_input = tf.placeholder(tf.float32, (1, 1, hps.num_embedding_nodes))\n",
    "    num_lstm_layers = []\n",
    "    for i in range(hps.num_lstm_layers):\n",
    "        num_lstm_layers.append(hps.num_lstm_nodes[i])\n",
    "        num_lstm_layers.append(hps.num_lstm_nodes[i])\n",
    "\n",
    "    num_hidden_states = sum(num_lstm_layers)\n",
    "    input_state = tf.placeholder(tf.float32, (1, num_hidden_states))\n",
    "    unpack_init_state = tf.split(input_state, num_lstm_layers, axis=1)\n",
    "    input_tuple_state = []\n",
    "    i = 0\n",
    "    while i < len(unpack_init_state):\n",
    "        input_tuple_state.append(\n",
    "            tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                unpack_init_state[i], unpack_init_state[i+1]))\n",
    "        i += 2\n",
    "    input_tuple_state = tuple(input_tuple_state)\n",
    "\n",
    "\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, 1.0)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        rnn_output, output_tuple_state = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            embed_input,\n",
    "            initial_state=input_tuple_state)\n",
    "        output_state = []\n",
    "        for state in output_tuple_state:\n",
    "            output_state.append(state[0])\n",
    "            output_state.append(state[1])\n",
    "        output_state = tf.concat(output_state, axis=1, name=\"output_state\")\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_output_2d = tf.reshape(rnn_output, [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_output_2d, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, 1.0)\n",
    "        fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout, vocab_size, name='logits')\n",
    "\n",
    "    return embed_input, rnn_output, logits, input_state, output_state, num_hidden_states\n",
    "\n",
    "\n",
    "img_feature, embed_img = eval_get_embedding_for_img(hps, img_feature_dim)\n",
    "word, embed_word = eval_embedding_lookup(hps, vocab_size)\n",
    "embed_input, rnn_output, logits, input_state, output_state, num_hidden_states = eval_lstm_single_step(hps, vocab_size)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    logging.info(\"[*] Reading checkpoint ...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(output_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(output_dir, ckpt_name))\n",
    "        logging.info(\"[*] Success Read Checkpoint From %s\" % (ckpt_name))\n",
    "    else:\n",
    "        raise Exception(\"[*] Failed load checkpoint\")\n",
    "    \n",
    "    for i in range(test_examples):\n",
    "        single_img_features, single_sentence_ids, single_weights, single_img_name = caption_data.next(hps.batch_size)\n",
    "        print(single_img_name)\n",
    "\n",
    "        pprint.pprint(img_name_to_tokens[single_img_name[0]])\n",
    "        pprint.pprint(img_name_to_token_ids[single_img_name[0]])\n",
    "\n",
    "        embed_img_val = sess.run(embed_img, feed_dict={img_feature: single_img_features})\n",
    "\n",
    "        state_val = np.zeros((1, num_hidden_states))\n",
    "        embed_input_val = embed_img_val\n",
    "        generated_sequence = []\n",
    "\n",
    "        for j in range(hps.num_timesteps):\n",
    "            logits_val, state_val = sess.run([logits, output_state],\n",
    "                                             feed_dict = {\n",
    "                                                 embed_input: embed_input_val,\n",
    "                                                 input_state: state_val\n",
    "                                             })\n",
    "            predicted_word_id = np.argmax(logits_val[0])\n",
    "            generated_sequence.append(predicted_word_id)\n",
    "            embed_input_val = sess.run(embed_word,\n",
    "                                       feed_dict={word: [[predicted_word_id]]})\n",
    "        pprint.pprint(\"generated words: \")\n",
    "        pprint.pprint(generated_sequence)\n",
    "        pprint.pprint(vocab.decode(generated_sequence))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
